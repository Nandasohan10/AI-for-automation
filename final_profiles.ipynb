{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder structure created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder structure\n",
    "folder_structure = {\n",
    "    \"selenium_pytest_project\": {\n",
    "        \"configurations\": [],\n",
    "        \"page_objects\": [],\n",
    "        \"test_cases\": [],\n",
    "        \"test_data\": [],\n",
    "        \"utilities\": [],\n",
    "        \"reports\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to create folders\n",
    "def create_folders(base_path, folder_structure):\n",
    "    for folder, subfolders in folder_structure.items():\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        \n",
    "        # Create the main project folder\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"✅ Created folder: {folder_path}\")\n",
    "        \n",
    "        # Create subfolders inside each main folder\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(folder_path, subfolder)\n",
    "            if not os.path.exists(subfolder_path):\n",
    "                os.makedirs(subfolder_path)\n",
    "                print(f\"✅ Created subfolder: {subfolder_path}\")\n",
    "\n",
    "# Base path where the project folder will be created\n",
    "base_path = os.getcwd()  # Current working directory\n",
    "\n",
    "# Create the folder structure\n",
    "create_folders(base_path, folder_structure)\n",
    "\n",
    "print(\"✅ Folder structure created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_cognito_access_token(client_id, client_secret, token_url):\n",
    "    # Prepare the data for the token request\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "    }\n",
    "\n",
    "    # Make the POST request to get the token\n",
    "    response = requests.post(token_url, data=data, auth=(client_id, client_secret))\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        token_info = response.json()\n",
    "        return token_info['access_token']\n",
    "    else:\n",
    "        # Handle error\n",
    "        raise Exception(f\"Failed to get token: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Replace these with your Cognito App Client ID and Secret\n",
    "client_id = '1m2f2ib1qatacreojll4aommfo'\n",
    "client_secret = 'o30f697g1s7fm9jrrlveeiu4tok3aulpie90709g86gijvccc2n'\n",
    "\n",
    "# Replace this with your Cognito token endpoint (Stg or Prod)\n",
    "token_url = 'https://logiai-stg.auth.us-west-2.amazoncognito.com/oauth2/token'\n",
    "\n",
    "access_token = get_cognito_access_token(client_id, client_secret, token_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction the required html elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder 'chunks_for_profiles' created.\n",
      "✅ Chunk 1 saved to: chunks_for_profiles\\beta-marketPlace_for_testing_chunk_1.html\n",
      "✅ Chunk 2 saved to: chunks_for_profiles\\beta-marketPlace_for_testing_chunk_2.html\n",
      "✅ Chunk 3 saved to: chunks_for_profiles\\beta-marketPlace_for_testing_chunk_3.html\n",
      "✅ Chunk 4 saved to: chunks_for_profiles\\beta-marketPlace_for_testing_chunk_4.html\n",
      "✅ Chunk 5 saved to: chunks_for_profiles\\beta-marketPlace_for_testing_chunk_5.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Paths\n",
    "input_html_path = 'raw_profiles.html'  # Original HTML\n",
    "output_folder = 'chunks_for_profiles'  # Folder to save the chunks\n",
    "\n",
    "# Function to create folder if it doesn't exist\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"✅ Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"✅ Folder '{folder_path}' already exists.\")\n",
    "\n",
    "# Function to extract meaningful testing elements and split into chunks\n",
    "def extract_and_chunk_elements(input_html_path, output_folder, chunk_size=50):\n",
    "    with open(input_html_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the full HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Create a new BeautifulSoup object for the cleaned output\n",
    "    new_soup = BeautifulSoup(\"<html><body></body></html>\", 'html.parser')\n",
    "\n",
    "    # Elements important for Selenium Testing\n",
    "    important_tags = []\n",
    "\n",
    "    # --- Form Controls ---\n",
    "    important_tags += soup.find_all(['input', 'button', 'select', 'textarea'])\n",
    "\n",
    "    # --- Links that can navigate ---\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'] != '#' and not link['href'].startswith('javascript'):\n",
    "            important_tags.append(link)\n",
    "\n",
    "    # --- Headings ---\n",
    "    important_tags += soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # --- List items ---\n",
    "    important_tags += soup.find_all('li')\n",
    "\n",
    "    # --- Optional: Divs or spans that contain visible text or action ---\n",
    "    for tag in soup.find_all(['div', 'span']):\n",
    "        if tag.get_text(strip=True):  # Only if they have visible text\n",
    "            important_tags.append(tag)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    create_folder(output_folder)\n",
    "\n",
    "    # Split into chunks\n",
    "    total_elements = len(important_tags)\n",
    "    chunk_count = (total_elements // chunk_size) + (1 if total_elements % chunk_size else 0)\n",
    "\n",
    "    for i in range(chunk_count):\n",
    "        chunk_start = i * chunk_size\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "\n",
    "        # Create a new soup object for the chunk\n",
    "        chunk_soup = BeautifulSoup(\"<html><body></body></html>\", 'html.parser')\n",
    "\n",
    "        # Append a chunk of elements to the body\n",
    "        for tag in important_tags[chunk_start:chunk_end]:\n",
    "            chunk_soup.body.append(tag)\n",
    "\n",
    "        # File path for the chunk\n",
    "        chunk_file_path = os.path.join(output_folder, f\"beta-marketPlace_for_testing_chunk_{i+1}.html\")\n",
    "\n",
    "        # Write the chunk to a new HTML file\n",
    "        with open(chunk_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(chunk_soup.prettify())\n",
    "        \n",
    "        print(f\"✅ Chunk {i+1} saved to: {chunk_file_path}\")\n",
    "\n",
    "# Run the extraction and chunking\n",
    "extract_and_chunk_elements(input_html_path, output_folder, chunk_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created Page Object: C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\\beta-marketPlace_for_testing_chunk_1.py\n",
      "✅ Created Page Object: C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\\beta-marketPlace_for_testing_chunk_2.py\n",
      "✅ Created Page Object: C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\\beta-marketPlace_for_testing_chunk_3.py\n",
      "✅ Created Page Object: C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\\beta-marketPlace_for_testing_chunk_4.py\n",
      "✅ Created Page Object: C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\\beta-marketPlace_for_testing_chunk_5.py\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Function to extract code from OpenAI response\n",
    "def extract_code(response_text):\n",
    "    code_blocks = response_text.split(\"```python\")\n",
    "    if len(code_blocks) > 1:\n",
    "        return code_blocks[1].split(\"```\")[0].strip()\n",
    "    return response_text.strip()\n",
    "\n",
    "# Function to read all chunk files from a folder\n",
    "def read_chunks_from_folder(chunks_folder):\n",
    "    chunk_files = []\n",
    "    for filename in os.listdir(chunks_folder):\n",
    "        file_path = os.path.join(chunks_folder, filename)\n",
    "        if filename.endswith(\".html\") or filename.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                chunk_content = file.read()\n",
    "                chunk_files.append((filename, chunk_content))\n",
    "    return chunk_files\n",
    "\n",
    "# Function to generate Page Object files\n",
    "def generate_pom_from_chunks(project_folder, chunks_folder):\n",
    "    # Define page_objects folder path\n",
    "    page_objects_folder = os.path.join(project_folder, \"page_objects\")\n",
    "    os.makedirs(page_objects_folder, exist_ok=True)\n",
    "\n",
    "    # Read all chunks\n",
    "    chunks = read_chunks_from_folder(chunks_folder)\n",
    "    if not chunks:\n",
    "        print(f\"❌ No valid HTML chunk files found in {chunks_folder}\")\n",
    "        return\n",
    "\n",
    "    # OpenAI client setup\n",
    "    client = openai.OpenAI(\n",
    "        base_url=\"https://stg-logiai-service.np.logitech.io/openai/v1\",\n",
    "        api_key=access_token  # Make sure this is already available\n",
    "    )\n",
    "\n",
    "    for filename, chunk_content in chunks:\n",
    "        # Create a simple prompt for each chunk\n",
    "        prompt = f\"\"\"\n",
    "        I am creating a Selenium Page Object Model (POM) for a webpage.\n",
    "\n",
    "        Below is an HTML snippet:\n",
    "        {chunk_content}\n",
    "\n",
    "        Based on this HTML, please generate a Python Page Object Class:\n",
    "        - Class name should be based on the file name (without extension), in PascalCase.\n",
    "        - Use Selenium locators and best practices.\n",
    "        - Create `__init__` method and basic element locators.\n",
    "        - No test cases — only page elements and actions if obvious.\n",
    "        - Use `By` from Selenium.\n",
    "        - Return the code inside a Python code block.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=False,\n",
    "                max_tokens=1200,\n",
    "            )\n",
    "\n",
    "            # Extract the code\n",
    "            pom_code = extract_code(completion.choices[0].message.content)\n",
    "\n",
    "            # Convert filename to class name\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            class_name = \"\".join(word.capitalize() for word in base_name.split(\"_\"))\n",
    "\n",
    "            # Save file\n",
    "            pom_file_path = os.path.join(page_objects_folder, f\"{base_name}.py\")\n",
    "            with open(pom_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(pom_code)\n",
    "\n",
    "            print(f\"✅ Created Page Object: {pom_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n",
    "# === Main Run ===\n",
    "# Input paths\n",
    "waste_folder = r\"C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\"\n",
    "chunks_folder = r\"C:\\genai\\ai_for_automation\\chunks_for_profiles\"\n",
    "\n",
    "# Run function\n",
    "generate_pom_from_chunks(waste_folder, chunks_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimized file created at C:\\genai\\ai_for_automation\\selenium_pytest_project\\page_objects\\page_objects_profile.py\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "def read_all_chunks_from_folder(folder_path):\n",
    "    \"\"\"Reads and combines all .py files from the given folder.\"\"\"\n",
    "    combined_content = \"\"\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".py\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                combined_content += f.read() + \"\\n\\n\"\n",
    "    return combined_content\n",
    "\n",
    "def extract_code(response_text):\n",
    "    \"\"\"Extracts Python code from markdown-style response.\"\"\" \n",
    "    if \"```python\" in response_text:\n",
    "        return response_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    return response_text.strip()\n",
    "\n",
    "def send_to_llm(file_content, api_key, base_url):\n",
    "    \"\"\"Sends the combined content to the LLM and returns optimized code.\"\"\"\n",
    "    client = openai.OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a Python Selenium expert following the Page Object Model (POM).\\n\"\n",
    "        \"Below is a class split across chunks. Please:\\n\"\n",
    "        \"- Combine all chunks into one clean class.\\n\"\n",
    "        \"- Keep ALL functions (do not remove any).\\n\"\n",
    "        \"- Do NOT change any locators.\\n\"\n",
    "        \"- Remove duplicate function names.\\n\"\n",
    "        \"- Ensure class is clean and well-structured.\\n\"\n",
    "        \"- go through every function if there is any logic mistake try to write a comment after the function. \\n\"\n",
    "        \"- try to create all the function in one single class\"\n",
    "        f\"{file_content}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False,\n",
    "            max_tokens=9000,\n",
    "        )\n",
    "        return extract_code(completion.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error from LLM: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_output_file(output_file_path, optimized_content):\n",
    "    \"\"\"Writes optimized code to the specified output file.\"\"\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(optimized_content)\n",
    "        print(f\" Optimized file created at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "\n",
    "def process_folder(folder_path, output_file_path, api_key, base_url):\n",
    "    \"\"\"Reads all chunk files, combines them, sends to LLM, and writes result.\"\"\"\n",
    "    file_content = read_all_chunks_from_folder(folder_path)\n",
    "    if not file_content:\n",
    "        print(f\"No Python files found in: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    optimized_content = send_to_llm(file_content, api_key, base_url)\n",
    "    if not optimized_content:\n",
    "        print(\"No optimized content returned.\")\n",
    "        return\n",
    "\n",
    "    create_output_file(output_file_path, optimized_content)\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"C:\\genai\\ai_for_automation\\wastage\\pom_chunks_profiles\\page_objects\"\n",
    "output_file = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\page_objects\\page_objects_profile.py\"\n",
    "api_key = access_token  # Replace or define this\n",
    "base_url = \"https://stg-logiai-service.np.logitech.io/openai/v1\"\n",
    "\n",
    "process_folder(input_folder, output_file, api_key, base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing chunk 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test file saved at: C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_final_profiles.py\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# === Utility Functions ===\n",
    "\n",
    "def read_files_from_folder(folder_path):\n",
    "    contents = []\n",
    "    try:\n",
    "        for filename in sorted(os.listdir(folder_path)):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    contents.append(file.read())\n",
    "        return contents\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading files from folder: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_code(response_text):\n",
    "    if \"```python\" in response_text:\n",
    "        return response_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    return response_text.strip()\n",
    "\n",
    "def send_to_llm_single_chunk(html_chunk, pom_code, conftest_code, api_key, base_url):\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an expert in Selenium and Pytest.\\n\"\n",
    "        \"Generate a Python test function using pytest and the provided conftest fixture and Page Object Model (POM).\\n\"\n",
    "        \"Use correct imports and create only testcases and import the the page object.\\n\"\n",
    "        \"user setup method for launching a driver once and start testing \"\n",
    "        f\"HTML Snippet:\\n{html_chunk}\\n\\n\"\n",
    "        f\"POM Code:\\n{pom_code}\\n\\n\"\n",
    "        f\"conftest.py:\\n{conftest_code}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=4096\n",
    "        )\n",
    "        return extract_code(completion.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"LLM call failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def save_combined_test(output_file_path, test_code):\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(test_code)\n",
    "        print(f\"Final test file saved at: {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final test file: {e}\")\n",
    "\n",
    "# === Main Function ===\n",
    "\n",
    "def generate_single_test_file(html_folder, pom_file_path, conftest_path, output_file_path, api_key, base_url):\n",
    "    html_chunks = read_files_from_folder(html_folder)\n",
    "    pom_code = read_file(pom_file_path)\n",
    "    conftest_code = read_file(conftest_path)\n",
    "\n",
    "    combined_test_code = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(html_chunks):\n",
    "        print(f\"\\n--- Processing chunk {i+1}/{len(html_chunks)} ---\")\n",
    "        test_code = send_to_llm_single_chunk(chunk, pom_code, conftest_code, api_key, base_url)\n",
    "        if test_code:\n",
    "            combined_test_code += f\"# === Test from chunk {i+1} ===\\n{test_code}\\n\\n\"\n",
    "        else:\n",
    "            print(f\"No test code received for chunk {i+1}.\")\n",
    "\n",
    "    if combined_test_code:\n",
    "        save_combined_test(output_file_path, combined_test_code)\n",
    "    else:\n",
    "        print(\"No test code generated.\")\n",
    "\n",
    "\n",
    "# === Update these with correct paths ===\n",
    "\n",
    "html_folder = r\"C:\\genai\\ai_for_automation\\chunks_for_profiles\"\n",
    "pom_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\page_objects\\page_objects_profile.py\"  # Use correct single POM file\n",
    "conftest_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\configurations\\configurations\\conftest.py\"\n",
    "output_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_final_profiles.py\"\n",
    "api_key = access_token\n",
    "base_url = \"https://stg-logiai-service.np.logitech.io/openai/v1\"\n",
    "\n",
    "generate_single_test_file(html_folder, pom_file_path, conftest_path, output_file_path, api_key, base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://stg-logiai-service.np.logitech.io/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__: Optimized file created at C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_case_profiles.py\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_single_file(file_path):\n",
    "    \"\"\"Reads and returns the content of a single file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_code(response_text):\n",
    "    \"\"\"Extracts Python code from markdown-style response.\"\"\" \n",
    "    if \"```python\" in response_text:\n",
    "        return response_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    return response_text.strip()\n",
    "\n",
    "def send_to_llm(file_content, api_key, base_url):\n",
    "    \"\"\"Sends a single file's content to the LLM and returns optimized Python code.\"\"\"\n",
    "    client = openai.OpenAI(\n",
    "        base_url=base_url,\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a Selenium expert using Pytest.\n",
    "\n",
    "You will be given a merged raw test case file composed of multiple test chunks. Your task is to:\n",
    "\n",
    "1. Consolidate the code into a **single clean test file**.\n",
    "2. **Do NOT remove or rename any test functions** — all test logic must be preserved.\n",
    "3. **Eliminate duplicate imports** and organize them properly.\n",
    "4. Remove **duplicate test cases** (same name and logic).\n",
    "5. Ensure that **each test function is unique**, well-structured, and properly formatted.\n",
    "6. Clean and organize fixtures and helper methods without changing their behavior.\n",
    "\n",
    "IMPORTANT:\n",
    "- **Preserve test logic and data** from all chunks.\n",
    "- Ensure **all test cases and fixtures are retained** unless they are exact duplicates.\n",
    "- Adjust **import paths consistently** (e.g., unify `from POM` vs `from pom` vs `from plugin_marketplace_page`).\n",
    "- Ensure **no syntax errors** and that the file is ready to run with `pytest`.\n",
    "\n",
    "Return a single, clean, final test file with all necessary test functions and imports.\n",
    "\n",
    "Here is the raw test file:\n",
    "\n",
    "{file_content}\n",
    "\"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False,\n",
    "            max_tokens=9000,\n",
    "        )\n",
    "        return extract_code(completion.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while sending content to LLM: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_output_file(output_file_path, optimized_content):\n",
    "    \"\"\"Writes optimized code to output file.\"\"\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(optimized_content)\n",
    "        logger.info(f\" Optimized file created at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to output file: {e}\")\n",
    "\n",
    "def process_single_file(input_file_path, output_file_path, api_key, base_url):\n",
    "    \"\"\"Main function for processing a single file.\"\"\"\n",
    "    file_content = read_single_file(input_file_path)\n",
    "    if not file_content:\n",
    "        logger.warning(f\"No content found in file: {input_file_path}\")\n",
    "        return\n",
    "\n",
    "    optimized_content = send_to_llm(file_content, api_key, base_url)\n",
    "    if not optimized_content:\n",
    "        logger.warning(\"No optimized content received from LLM.\")\n",
    "        return\n",
    "\n",
    "    create_output_file(output_file_path, optimized_content)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_final_profiles.py\"\n",
    "output_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_case_profiles.py\"\n",
    "api_key = access_token  # Make sure access_token is defined or imported\n",
    "base_url = \"https://stg-logiai-service.np.logitech.io/openai/v1\"\n",
    "\n",
    "process_single_file(input_file_path, output_file_path, api_key, base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Read HTML chunk from file: beta-marketPlace_for_testing_chunk_1.html\n",
      "INFO:__main__:Read HTML chunk from file: beta-marketPlace_for_testing_chunk_2.html\n",
      "INFO:__main__:Read HTML chunk from file: beta-marketPlace_for_testing_chunk_3.html\n",
      "INFO:__main__:Read HTML chunk from file: beta-marketPlace_for_testing_chunk_4.html\n",
      "INFO:__main__:Read HTML chunk from file: beta-marketPlace_for_testing_chunk_5.html\n",
      "WARNING:__main__:Input file content exceeds 6000 characters. Truncating...\n",
      "ERROR:__main__:Error while sending content to LLM: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "WARNING:__main__:No optimized content received from LLM.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# ============ Logging Configuration ============\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============ Utility Functions ============\n",
    "\n",
    "def read_single_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads the content of a single file and returns the content as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def read_html_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all HTML files from a specified folder and combines their contents into a single string.\n",
    "    \"\"\"\n",
    "    combined_html = []\n",
    "    try:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_name.endswith(\".html\"):  # Only process HTML files\n",
    "                html_chunk = read_single_file(file_path)\n",
    "                if html_chunk:\n",
    "                    combined_html.append(html_chunk)\n",
    "                    logger.info(f\"Read HTML chunk from file: {file_name}\")\n",
    "        return \"\\n\".join(combined_html)  # Combine all chunks into a single string\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading HTML files from folder {folder_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def truncate_file_content(file_content, max_length=6000):\n",
    "    \"\"\"\n",
    "    Truncates file content to a specified length to avoid token limit issues.\n",
    "    Appends a comment indicating truncation.\n",
    "    \"\"\"\n",
    "    if len(file_content) > max_length:\n",
    "        logger.warning(f\"Input file content exceeds {max_length} characters. Truncating...\")\n",
    "        return file_content[:max_length] + \"\\n# TRUNCATED CONTENT DUE TO TOKEN LIMIT\"\n",
    "    return file_content\n",
    "\n",
    "def extract_code(response_text):\n",
    "    \"\"\"\n",
    "    Extracts Python code blocks from a model's markdown response.\n",
    "    If no `python` code block is found, returns the raw response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"```python\" in response_text:\n",
    "            return response_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting Python code: {e}\")\n",
    "    return response_text.strip()\n",
    "\n",
    "def create_output_file(output_file_path, optimized_content):\n",
    "    \"\"\"\n",
    "    Writes the provided content to a file at the specified path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(optimized_content)\n",
    "        logger.info(f\"Optimized file successfully created at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to file {output_file_path}: {e}\")\n",
    "\n",
    "\n",
    "# ============ Core Functionality ============\n",
    "def send_to_llm(file_content, api_key, base_url, html_chunks, pom_code=\"\", conftest_code=\"\"):\n",
    "    \"\"\"\n",
    "    Sends the file content to the language model (LLM) for processing\n",
    "    and returns a Python code file optimized for Selenium and Pytest.\n",
    "    \"\"\"\n",
    "    # Set OpenAI API configurations\n",
    "    openai.api_key = api_key\n",
    "    openai.api_base = base_url\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in Selenium and Pytest.\n",
    "\n",
    "    Using **Page Object Model (POM)**, generate Python test cases for testing a web application's functionality. \n",
    "    Focus on the following:\n",
    "\n",
    "    1. **Automation Context:** Write test cases to validate user interactions, navigation, buttons, links, or input fields.\n",
    "    2. Use the given **POM class and conftest fixture** to structure the test cases.\n",
    "    3. Consolidate the code from the given raw test file into a **single consolidated Python test file**.\n",
    "    4. Optimize execution by ensuring the browser launches **only once using a session/module-level fixture**. \n",
    "    5. Create output with:\n",
    "        - Well-named test functions, meaningful docstrings, and proper assertions.\n",
    "        - Reuse of test setup fixtures for all test cases.\n",
    "        - Readable, clean, and modular Python design.\n",
    "\n",
    "    Inputs:\n",
    "    - **Combined HTML Snippets**: \\n{html_chunks}\n",
    "    - **Page Object Model (POM) Code**: \\n{pom_code}\n",
    "    - **conftest.py Code**: \\n{conftest_code}\n",
    "\n",
    "    Here is the raw test file for consolidation and optimization:\\n{file_content}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Send request to OpenAI API\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=3000,  # Adjust token limit based on needs\n",
    "            temperature=0  # For deterministic results\n",
    "        )\n",
    "        # Extract and return optimized code\n",
    "        return extract_code(response.choices[0].message[\"content\"])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while sending content to LLM: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_single_file(input_file_path, output_file_path, html_folder_path, api_key, base_url, pom_code, conftest_code):\n",
    "    \"\"\"\n",
    "    Main function to process a single test case file with the LLM.\n",
    "    Reads the input file, sends it to the LLM for optimization, and writes the output.\n",
    "    Combines HTML files from a folder into the prompt.\n",
    "    \"\"\"\n",
    "    file_content = read_single_file(input_file_path)\n",
    "    if not file_content:\n",
    "        logger.warning(f\"No content found in file: {input_file_path}\")\n",
    "        return\n",
    "\n",
    "    # Read all HTML chunks from the folder\n",
    "    html_chunks = read_html_from_folder(html_folder_path)\n",
    "    if not html_chunks:\n",
    "        logger.warning(f\"No HTML chunks found in folder: {html_folder_path}\")\n",
    "        return\n",
    "\n",
    "    # Truncate content if necessary\n",
    "    truncated_content = truncate_file_content(file_content)\n",
    "\n",
    "    # Send the content to LLM\n",
    "    optimized_content = send_to_llm(truncated_content, api_key, base_url, html_chunks, pom_code, conftest_code)\n",
    "    if not optimized_content:\n",
    "        logger.warning(\"No optimized content received from LLM.\")\n",
    "        return\n",
    "\n",
    "    # Create an output file with the optimized code\n",
    "    create_output_file(output_file_path, optimized_content)\n",
    "\n",
    "\n",
    "# ============ Example Usage ============\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file paths\n",
    "    input_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_final_profiles.py\"\n",
    "    output_file_path = r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\test_cases\\test_case_profiles.py\"\n",
    "    html_folder_path = r\"C:\\genai\\ai_for_automation\\chunks_for_profiles\"  # Folder containing HTML files\n",
    "\n",
    "    # Define API Key and Base URL\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\", \"your_openai_api_key_here\")  # Avoid hardcoding API keys\n",
    "    base_url = \"https://stg-logiai-service.np.logitech.io/openai/v1\"  # Replace with actual URL if applicable\n",
    "\n",
    "    # Define the additional inputs for the prompt\n",
    "    pom_code = read_single_file(r\"C:\\genai\\ai_for_automation\\selenium_pytest_project\\page_objects\\page_objects_profile.py\")  # Adjust path\n",
    "    conftest_code = read_single_file(r\"selenium_pytest_project/configurations/configurations/conftest.py\")  # Adjust path\n",
    "\n",
    "    # Process the file\n",
    "    process_single_file(input_file_path, output_file_path, html_folder_path, api_key, base_url, pom_code, conftest_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
